---
talk_id: css-and-the-first-meaningful-paint
---

TIM:  Our next speaker's first Tweet was about donuts, there you go. Patrick has been talking about web performance and specifically trying to get the web page to show up as fast as possible. In 2014, you would have seen him talk about the critical path. So much has changed in the last couple of years, especially with tools, tooling, and like kind of like helping everybody see these things, so he's kind of a veteran in this which is special, I think. I'm looking forward to it.
PATRICK:  Hello!  [Applause].  So, thank you very much for having me. It is a pleasure to be back here in Berlin as Tim just said, this is my second time on this stage, and it's a conference that, as everyone as already mentioned this morning ing with , has a community that is dear to my heart and so it is an honour to be able to be back here speaking again, so thank you, Krissy, Polly and the team and be , and allowing me to bring my child here. I've got a one-year-old baby here called Freddy. Come and say hello to him afterwards. I'm trying to get him on the CSS party pack later to do some object noxious animations. My name is Patrick Hamann. You can catch hey on Twitter there. I work at Fastly. We are an Edge cloud provider that specialises in real time content delivery. My role there is a web-performance engineer where I spend a lot of time thinking about how we can improve the speed of one, our network, but most importantly delivering content for our customers and users as fast as possible, some of which of that I'm going to talk about today and show you the research I've been doing.
So, why am I here?  You're probably wondering what the title "CSS and the first meaningful paint" even means and hopefully all will come true shortly. I'm going to ask you a question of how do you measure the performance of your website?  What does being fast even mean?  Is it how long it takes to get to a load event?  Is it how many bytes you send down the wire or how many requests?  But I would argue that is shouldn't be any of these things. Ultimately, we are building websites for our users or customer's users and it's about how they perceive the speed of your website. Should we even have a golden performance metric?  Does it even performance?  I would argue that it shouldn't. And, for years, we've been optimising our pages to be built for these metrics that don't - they directly correlate to how our websites are built and you  but not the user experience or how the users first experience these things - load event, the request I sent. None of these correlate to a good user experience. I can't stress enough, at the end of the day, we are building our websites for real users and they're coming here to do something. Unfortunately, we are seeing a rise of a whole new collection of metrics focused around user experience, such as the speed index and the first meaningful paint which we are going to talk about today and time for interactivity at present . We are also seeing rise of thinking about custom metrics specific to our needs, maybe for search results on a search page, it is how long it took the user to be able to find what they were looking for, or a news organisation. It is how long they took the user to find an article and read it. We should be thinking about metrics specific to our user's needs and not focused on how we delivered and built our technology. We're going to focus a bit on the first meaningful paint today.
What does that even mean?  To put is simply, the first meaningful paint is the time when a page's primary content appeared on the screen, i.e., the time when the thing the user came to the website appeared on the screen. More detailed, it is the first paint after which the - woo only care with the view port. If you can imagine on a mobile screen, that's only what appeared in the first view port, has happened. So layout is when the browser converts all of your elements and your CSS queries into an X and Y co-ordinate, i.e., where that element is going to be painted on the screen and its width and height. The browser has to perform layout of all the elements and paint them to the screen.  And most importantly, TTFMP also takes into consideration when custom web fonts have loaded, because I've heard that we all like to add custom web fonts to our websites these days, much to my disgruntlement and disagreement. Maybe this is better represented visually:  what does TTFMP actually mean?  It is a very new metric.  We've only started using it last year. Some folks at Google wrote a white paper - I will release the slides later - where we can go about ascertaining the time to first meaningful paint. It is not exposed to a JavaScript API yet but we are talking about it. You have to be able to use for instance Chrome's internal tracing to be able to do this, but on the top here, we have a graph that represents how many layout objects are actually painted to the screen, so that's time on the x-axis and the amount of time on the y axis and how that correlates to the user experience below. Google do something very clever that they flush the head of their document way before they even send the search queries back to the database, and so it paints very quickly on a 3G and emerging network, a paint of 1.5 seconds, but actually, the meaningful paint, the thing that the user actually came here for is at 1.9. We can see how that directly correlates to the amount of objects painted to the screen approximately hopefully, you've now got a much better indication of this is a metric that is much nor useful than start render or time to first byte because we've given the user what they came here for, and , and this is the type of metric BBC should be focus on, talking about, and building and trying to optimise for. Maybe this is better represented like this. This is the FT.com home page. Who thinks the time to first meaningful paint is 3.5 or 4.5?  5?  So, yeah, the TTFMP here is actually five seconds on a 3G connection. You're probably wondering how you can measure your TTPMP yourself?  Google open-sourced this lighthouse, an auditing tool how your new web app was performing against some of the best practices, but it turns out some of the audits in it are very good for any website. You don't have to use service workers or a progressive web app to use your tool. You can install it via a browser plug-in and a CLI and have it as part of your build process. You can expose the TTFMP of your own website. I urge you to go home and check this out on your own.
Let's dive in and how we can optimise our own websites for that TTFMP metric and how that relates and it's extremely tightly coupled to CSS, and is CSSconf. CSS is so critical to how we deliver a good user experience to our users. So, we are going to look at Pat, present, and future best perhaps for how we deliver CSS and the assets related to it in the browser and hopefully you will be able to apply tomorrow of these methodologies at home. To do, it is easy to create a test study, a to-do MVP app. It is much better I find to show real and use real websites to optimise for because ultimately, we are again all building real websites and not just a to do MVP app that delivers in two files, so we decide to throw a metric crack tonne of JavaScript down the pipe at the same time, and this is using real-world conditions is when we can see how the optimisations have effect. For the purpose of the talk and the research, I'm going to use the FT.com home page and apply some optimisations together to it see how we can provide the meaningful paint. I used to work for the Financial Times, I know how the pages are built. They've given me permission to do this. I think some of the developers who still work there are in the room!  But, you know, it is actually a very fast website, but we can probably improve it, and that's what we are going to do.  So for each optimisation, we will test it on a real device.  I can't stress this enough. We are going to use web-paste test. If you haven't used it before, I urge you to do it. It is the number-one tool box in my ... devices. There is a real MotoG sitting at the end of this form allowing me to shape to a real network condition that again we are building websites for real users, okay?  Contrary to popular belief, the shiny iPhone that you have in your pocket is not a real-world device, and actually the average device globally is a low- to mid- end Android so memory test allows us to test on these devices. We are going to run nine tests, choose the median result of them and choose three network conditions which I will chat about later. My friend Ben is going to be talking about this kind of data of real-world devices and what is the real network at JSConf this weekend so I urge you to look at his talk.  We need to ask ourselves these questions to ask what our average user profile is. Where are your uses based?  What is their device landscape?  In what context are they using your website?  I would be very surprised if one of you can tell me that you have a single context for your users, because that just doesn't exist. The same person might be accessing your website on a flakey mobile connection in the morning and then at lunchtime uses the same website on their fast iMac with the fast cable connection. There isn't this single context. What is anywhere network profile and what specifically did they come here for?  That's what you should be optimising for. For the purposes of the test, we will try to deliver a three-paint set on the emerging network. It is a four-milliseconds just on an emerging marking connection. We will try to optimise for 1,000 milliseconds on cable. You should not be taking my baselines as at home on what you should be using, you should be thinking about what your user profile is and setting budgets for those. So that we can measure the impact of our tests, and our optimisations, we need to set a baseline. So this hopefully will look very familiar to all of you. This is a an HTML document that right near the top we have a link element referencing our compiled main CSS file. I don't care how that was compiled, authored in JS or whatever, as long as you're delivering your CSS like this, this is how we've been delivering CSS since the birth of cascading style sheets. It's probably how host of you in the room are delivering your CSS. This is the waterfall result we got in page test. Hands up if you've seen network water falls before?  Use them on a daily basis. Awesome. Doesn't matter if you haven't. The x-axis here is time, and the y axis is the network request that the browser is making and the priority and order in which that happened. Web test page gives us MIME types here, JavaScript is orange and fonts are red. For each one of these segments, there are two shades of colour, when we sent the request, the waiting period and the download process and how long it took. Here, we can see that we requested our HTML file, it found the link element because it parsed the document incrementally - that's the great thing about HTML spec is that it can be parse the incrementally. It found the link element quite soon during the download of the HTML and triggered request. Then the green line here is our start render. That's not the time to first meaningful paint but when we started painting on a 3G emerging market. That gives us a baseline of around 8,000 milliseconds TTFMP on 3G and emerging market and around 2,000 for cable. So way out of our budget. Now we set a baseline. The first experiment we want to do is inline our critical CSS. Hopefully, many of you have heard this technique. This is what I came to this very stage to talk about in 2014. Since then, it is become quite a common practice. So let's have a quick recap of that. The first thing we need to do is we have to look at our critical rendering path of our page. This is the single path that a browser must do to be all the it has to do before it can paint to a screen. First, we have to make the get HTTP request index file. We get the response to that. We can start building the document object model, the DOM we interact with in JavaScript, the child-parent relationship, so I have a body tag which has a div, which has a P and a span. We can construct the DOM as we parse the - we can construct the DOM as we parse the HTML incrementally.  We don't have to wait for the bytes to be downloaded before we construct the DOM which is an amazing feature of the HTML specification. We find the link element, we then have to go and stop constructing our DOM because CSS is known as a render-blocking resource and we have to go and perform the networking for that, get that, and then we can construct our CSS object model which is the same as the DOM, but note the idle time here:  we have wasted a lot of time on our initiative connection waiting for that CSS file. On our network connection. Imagine if I was on a train, I clicked on the link to FT, I downloaded the HTML and then I go into a tunnel. Now I have no network and enough network to perform the CSS. In fact, I've got all the content. I could paint to the screen. What we've done here is created a single point of failure by delivering our CSS as a separate object. Whereas we do have all of the information. So the theory here is what if we were to inline the critical CSS required to render that first view port, the one we care about for our first meaning of the paint into the head of our document and then declare the rest of it as asynchronous telling the browser I don't care about this for the first paint. Now we have all the information we need probably within the first round trip of a network connection, so then we can construct the render tree and render to the page.
So, going back to our experiment, this is now what it looks like. We now have a style element in the head of our document. This goes against everything we've been taught about separation of concerns of styles in our style sheets, behaviour in our JavaScript and then we're using our new friend, the link pre-load element which we're going to talk about later and here I'm using the Filament Group load CSS for doing this. This is the best way for doing this at the moment. It has got a polyfill for browsers that don't support this. What if we were to run this in web page test?  This is our baseline, remember the start render line is there.  Now, with inline, look, we've instantly got painting as soon as we've started to parse the HTML document. We no longer have to wait for the CSS because we've declared that as asynchronous. You can imagine this is going to have a very dramatic effect on our time to meaningful paint metric. We have a 63 per cent improvement on our baseline, 1,300 milliseconds time to meaningful paint on cable and basically halved it which is extremely impressive using just one technique. This comes with some pros and cons. We no longer have the single point of failure. We've eliminated all of the blocking resources, but the eagle-eyed in the room would notice that probably because we've declared our other CSS as asynchronous, that still has to be loaded and we still have to apply it to be able to paint to the DOM, so it causes a reflow, and that's why it's really, really important that you separate your truly critical everything in that first view port from the non, because if you were to have stuff in the top of the view port, you're going to cause reflow and you have a jarring user experience. The most important point to note here, though, is we've made it not cacheable. By inlining the document, every time we change the CSS, we are essentially invalidating the cache for all of the HTML files. If you can imagine the FT.com, that's a very large cache we are going to be invalidating every time we change CSS. We don't benefit from the browser caching that object and we are going to be sending more bytes down the wire every time which is completely unnecessary. It is also very hard to maintain and very hard to automate inline CSS. I've worked on some large-scale websites and it's become a pain that we - there's no optimum way of doing this. We've been able to get our CSS down as soon as possible, but how can we then prioritise the delivery of the other sub resources that our CSS requires to achieve that time to first meaningful paint, i.e., the critical resources for our page?  I want to ask you a question about what can you think are the critical resources that you have on the website you were building yesterday or today, even for your customer or company, can you think of those resource s if the network failed, I only need these one, two, or three resources to be able to deliver a good user experience. The rest of them, ads, code for the JavaScript, that can all be left behind. Have a think about that. Let's identify the resources together for the FT.com page. Is it the logo?  Is it the custom web fonts?  We know that TTFMP has to use custom web fonts and we are delivering the news. The user needs to be able to read it. Is is it the hero image to give context. I'm sorry here for the treatment photo bomb. I've annoyed that I've included her in this. She doesn't deserve it!  How can we go about automating the pro set of choosing these critical resources?  Fortunately, again, lighthouse has got our back. They do this with an audit they call the "critical request chain". Here, running FT.com through a lighthouse we've developed a critical request chain of five resources. Even though I've asked you to work out what your resources are, you can automate this. If you were to take away one thing from this talk today, please let it be this:  that you need to be able to be optimising this critical request path. Eliminate every asset that is in it that is not critical to the core user experience of your website and then optimise the remaining ones. Make them as small as possible and prioritise the delivery of the assets over the network. That's how you will achieve a very fast time to first meaningful paint with your CSS.
So if we remember the methodology for ascertaining the time to first meaningful paint, we also block on our metric waiting for web fonts.  Because they are delivering the user experience after all.
So look how low down in our waterfall our web font delivery is:  much, much lower than the CSS. It's around the 20th request. Why is this?  If they are such an important critical resource, why is the browser prioritising that network request so low compared to some of our imagery?  To do this, we need to take a look back at how browsers go about painting again. First, the browser makes the network request for HTML documents, it begins parsing that document, it finds, discovers the CSS, maybe some JavaScript if you haven't declared it as asynchronous, and then CSS can't, unlike HTML, can't be parsed incrementally, i.e., we have to wait for all the bytes of the CSS to come in before we construct the object model, and there is a very good reason for that because of the nature of the cascade in cascading style sheets is you might have a style declaration at the bottom of your file that's overwritten it at the top. If we parsed it incrementally, you might have stuff moving around. It is a very good reason that CSS can't be parsed incrementally before we have to wait. We then get the CSS object model and these two trees form together to form the render tree. It is exactly like the DOM but it only contains the elements that are going to be displayed on the screen, because you might have other elements in your DOM that you've done "display none" on, you've hidden them. Why does the browser try and compute the layout objects for those when it's not going to paint them?  So the render tree is actually what is used to rasterise the page and send it. Why have I gone on this digress to tell you this?  It's only at this point that the font files are found and the networking for them is sent. And that's because you may have a whole block, a whole paragraph of text that has a font that you have declared but you've displayed none on that. Why should we go and perform the networking for that - font files by their pure nature are expensive, sometimes megabytes in size. Why should the browser perform the networking for them if they're not going to display them. It is for this reason that the browser doesn't perform networking for fonts until the render tree is constructed. You note here we've wasted an extremely large amount of time waiting for that period. What if we were able to hint to the browser, "These are my critical resources. You're not going to find them for ages because they are hidden sub resource inside the render tree, but I know as the author they're extremely important." If you imagine the five resources that lighthouse identified through the FT.com, they are the things we should be hinting to the browser. The working group has a new API called pre-load which allows as authors of the page to indicate to the browser the critical or hidden resources, so it can then prioritise the networking for them. To put simply, it provides a declarative fetch primitive for initiating an early fetch - and this is the most important bit - separating fetch from resource execution. So I can now say go and down load this JavaScript file but don't execute it because that will probably be bad, but perform the networking for it because it is extremely important delivery of my good user experience.
So this is what it looks like. We now have three new primitives in HTML at the top via the link element with a pre-load attribute. I think this is extremely powerful. We can do it dynamically with JavaScript.  The you can imagine if a user is hovering over a button to open up a carousel with images as at that point as they hover, you can inject link pre-load images so when they do click did on, you will have a lightning fast experience. My favourite, the link header. The often-forgotten http header. We are, with a http header on the response of our HTML file, we are telling the browser that go and pre-load the CSS file, it is extremely important. And so going back to our experiment on FT.com, we identified those five resources. We're now going to set the link pre-load header on the response of the HTML. There are two interesting points to note here that fonts have to be declared as cross-origin even if they're on the same origin or host because some genius person many years ago deemed that all fonts should be treated as cross-origin for security purposes, and the important thing is the no-push attribute here. I just want you to remember that for later on. Okay, so we've applied these link headers. What impact does this have on our time to first meaningful paint?  This is our waterfall before we apply they will, remember how the fonts are so low down, they're not discovered until the render tree construction. By applying the link headers, we've been able to tell the browser, these are my critical resources, and we've instantly shifted them up. Hopefully, you're now beginning to understand what impact this will have on our time to first meaningful paint. That's given us a 64 per cent improvement on that you are  our baseline now. We've hit the budget for cable of 1,000 milliseconds and we're very, very close now on 3G and emerging market connection.
So, again, this comes with some pros and cons. It gives us a way of indicating the hidden resources. We can now dictate the priority by order - sorry, that's a very important thing to note, is that the order of these headers also dictates their priority in which the browser will perform the networking for them, so does their relation of style and font.
And, the con here is easy to create contention on the network, with great power comes great responsibility.  If you were just to set a pre-load header for every single asset on your page, you're actually not allowing the browser to do what it's very good at doing, using its pre-load scanner or speculative parser to find and prioritise assets. Only use this technique for your critical resources that you have identified.
So we could stop there, right?  We've improved our by 64 per cent, but surely we can do more. This is where HTTP/2.0 and server-push comes into play. It is the first time in 20 years that we have the new version of the underlying transfer protocol of the internet called HTTP/2.0. I could do a whole talk on that - but I don't have the time - but I urge you to check it out. Who is using HTTP/2.0 in production?  Okay, so it's about 30 per cent of the room from the looks of things, which is lower than I was hoping, but that's still very good. That was a bit weird. I've broken an animation.
To understand what it does within let's look at how your average web page is constructed. First, we make the get request for the index file. The server responds with the index file, we parse the document, find the link element and make the request for the CSS file and the server responds with that. But what if we, the server, knows as the authors of the website, we know that the next item or resource that the client is going to request is that CSS file, because we know its had a the highest priority?  What as an author we could dictate to the client I'm going to push you the CSS file. You don't need to request it?  This is where the push-promise frame comes in. It is a data payload on an HTTP/2.0 connection but the server says I'm going to send you the bytes for the CSS file, you don't need to request this. Then we can push the index file and push the CSS down. We are eliminating the round trip completely to request that CSS file.
So now we can do this with HTTP/2.0 programmatically using our friend, the link header here.  This is the semantic we've decided on as an industry. If we go back to our FT.com example we can remove the inline CSS now and remove the style sheet and have the non-critical CSS as asynchronous and tell our HTTP/2.0 server you have to have an HTTP/2.0 server in order to be able to work and I want you to push the CSS and we removed in a no-push directive I told you about earlier on. An HTTP/2.0 would see this response, saying, "The author wants me to push the resources" and push the critical CSS bytes down.
Let's take a closer look at what is actually happening on the network before we apply this optimisation. And note that we have idle time before we get our HTML bytes, when the browser made the request and waiting for the server to respond to it at the beginning. Then we have a fast start render because we've inlined our critical CSS. Then the browser has some think time again. It then requests the main CSS file and there's the time to the first meaningful paint. I don't know what you think is going to happen here but you may be a little surprised as the impact of pushing our critical CSS. Removing it from inline and pushing it.
We have actually had a negative impact on the time to first meaningful paint. Why is this?  We are not using the idle time at the beginning of the connection. To understand why this is, we have to have a better understanding of of what is actually happening on the HTTP/2.0 server here. HTTP/2.0 servers use a prioritisation tree to determine which the order of which the packets it's going to send down the connection. Even if we told our HTTP/2.0 server to push the CSS via our link header, because we actually did it on the response of the HTML document, the serve remember had the HTML and HTML has a prior priority than CSS does so it will flush the HTML bytes down the connection before our critical CSS even though, what we actually wanted it to do, was flush the CSS in our idle time whilst we were waiting for our HTML connection. I must stress this behaviour is different depending on your HTTP/2.0 implementation but all of the HTTP/2.0 servers prioritise HTML over CSS on the connection - quite rightly. This is the impact it has on our waterfall. The interesting here to note is that the critical CSS doesn't have a light-green segment beforehand because it didn't need to send the request and therefore there was no waiting for it, so this is the benefit we get from push is that we are reducing the idle time from the request to the response and be , and we are not getting our CSS delivered at the point we want it to in the light-blue area of our HTML.
It is also important to note, if you weren't inlining your CSS, and many of you probably aren't, this is still quite a good technique because you're saving that request time.
So we have had a negative impact on our time to first meaningful paint here - only 43 per cent improvement on our baseline. We've gone back to 5,000 milliseconds.
So the question I'm asking is should we be using server push at all?  Actually, if we're having a negative impact?  Most importantly, is using the link header with a pre-load actually much too late in the connection state for us to indicate the CSS that we want you to push?  And so, how can we actually achieve that Holy Grail of pushing the CSS in idle time?  This is what at Fastly we are calling "async push", so let's look at that network utilisation again and note at the beginning we are having the idle time waiting for the server to respond, it's probably performing the templating, requesting the information from the-gate. It is this think time that we want to be using the static assets. A much more common architecture that probably many of you in the room are using is your application server is separate from your http server, you might use a CDN, for instance. This is the request flow that the index goes through and we get a response. What if that server was able, during an application think-time, was able to push the critical resource at that time?  This is the think-time that we want to be using. Here's an example of doing that using an express-style express handler using HTTP nodes, HTTP/2.0 implementation. The important thing to note isn't the implementation but what we are doing. At the beginning of the request, the first thing we are doing is flushing our critical CSS via push down the network connection, and then we go and do our templating and our fetching from our rendering and then we respond with our link headers. This is how we can utilise that idle connection time. What happens to our connection state here now is we've reached the Holy Grail. We've been able to push our critical CSS in the light-blue area while the server is still thinking. We've given the browser everything it needs, way before it even gets any HTML to style and paint your entire document.
So now we have a 65 per cent improvement on our baseline. We've reached that Holy Grail. This comes with some pros and cons that we are using that idle time, but it's easy to create contention with push and there's limited availability of this because you have to have access to the network connection within your server to be able to do this. Hopefully, many of you in the room are thinking what about the repeat view in every one of our tests has been the first view, the uncached, the browser doesn't have anything cached and that is what we should be optimising for for our time to first meaningful paint. If I was going to push the bytes, actually, the browser's probably got those assets cached already but we've got no way of indicating to the server that we have that CSS, so please contents push it  -- don't push it to me. I wanted to show you service workers if the purple pattern, but, unfortunately, I've ran out of time today. The good news is, Adios Mani will be talking about these patterns and how you can have time for the repeat view at JSConf this weekend. Let's look at the final results. It is better to show them to you as how the user is receiving this, with our async push, we've been able to improve the delivery on a 3G emerging market connection by over 3,000 milliseconds, which is a staggering improvement for our performance.
Okay, so finally, I wanted to leave you with a glitches of the future. We've now got a strong tool box of APIs, pre-load, HTTP push in inlining. There's issues with each of those which I outlined. The biggest weakness of server-push is by waiting that via a link pre-load header that is much too late in the networks congestion and this - we have a working draft in the ITF of the 103 early-hints status code. The 100 range, a lot of people didn't know existed, it is the information status-code range and we can now have this pattern where the client requests something from the server, and while the server is generating the response, it can flush an early hint response containing link headers to all of your critical sub resources and the browser can prioritise the networking for them.
Finally, the cache digest specification. Remember, I mentioned that the repeat view of push would be we have no way of indicating to the producer what we have in a cache or our CSS for that domain. Now, on an HTTP/2.0 connection, the client is going to be able to send via a cash digest frame or a header all of the files that it has for that host name and then the server can decide I'm not going to extend you that CSS file because I know you already have it in its cache. This in itself is going to be one of the most powerful performance optimisations in the future of the next five years as it gets implemented in browsers.
So, that's been a whirlwind tour. I've gone five minutes over my slot. I'm very sorry, my lovely CSS conference organisers. I want to leave you with some last points and takeaways that hopefully you found that resource loading in the browser is hard - it's much harder than we probably thought it was. Bandwidth is often under utilised but we have technologies emerging to give that utilisation to the browser. Identify your critical resources and your request chains. Use pre-load to indicate those critical resources to the browser, especially your fonts if you're using custom web fonts, and push your critical CSS but only in first view, and please try and only use within that idle connection time. Most importantly, always be testing. It is the most important thing to leave you with. Thank you very much. [Applause].
JESSICA:  Thank you, Patrick. That was awesome. That was such a great look into the kind of user experiences we are creating. Here's my segue. We love user experiences here too which is why it is the coffee break right now, so you have some time to go and get some coffee but I want to let you know over in this corner, there's a frozen yoghurt truck, and there is the community area, so you can talk to people about library, and learning HTML, CSS and there are a bunch of great people over there who will talk to you about the stuff they're working on. There is a bag-printing station there later. Please explore, go visit the sponsors, the community area and come back here, and, if you're standing, there are lots of seats up in the front. When you come back, you can fill those in. Thank you!
[Break]
